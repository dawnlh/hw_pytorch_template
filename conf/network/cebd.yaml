# @package _global_
network_name: cebd
arch:
  _target_: srcs.model.cebd_model.CEBDNet
  sigma_range: ${sigma_range}
  test_sigma_range: ${test_sigma_range}
  ce_code_n: 8
  frame_n: ${frame_n}
  ce_code_init: [1,1,1,0,0,1,0,1] # len8-03
  opt_cecode: false
  ce_net: CEBlurNet
  binary_fc: STEBinary_fc
  bd_net: BDNeRV_RC # BDNeRV | BDNeRV_RC | BDNeRV_RC2 | BDNeRV_BRC

loss:
  # _target_: srcs.loss._pix_loss_func.l1_loss
  _target_: srcs.loss._pix_loss_cls.WeightedLoss
  loss_conf_dict: {'CharbonnierLoss':1.0, 'EdgeLoss':0.05, 'fftLoss':0.01}
optimizer:
  # _target_: srcs.optimizer.adan.Adan
  # lr: !!float 5e-4
  _target_: torch.optim.Adam
  lr: 2e-4
  weight_decay: 0
  amsgrad: true
lr_scheduler:
  _target_: srcs.scheduler._base_scheduler.getGradualWarmupScheduler
  multiplier: 1
  warmup_epochs: 2
  after_scheduler_conf:
    type: torch.optim.lr_scheduler.CosineAnnealingLR
    args:
      T_max: ${trainer.epochs}
      eta_min: 1e-6
  # _target_: torch.optim.lr_scheduler.StepLR
  # step_size: 40
  # gamma: 0.9